import docx2txt
import itertools
import pandas as pd
from difflib import SequenceMatcher
import re
from rapidfuzz import fuzz
from rapidfuzz import process
from nltk.tokenize import PunktSentenceTokenizer

text = docx2txt.process('unidade 23 HAZOP LOPA (1).docx')
sent_tokenizer = PunktSentenceTokenizer(text)
sents = sent_tokenizer.tokenize(text)
sents = set(sents)
x_list = []
y_list = []
score = []

for x,y in itertools.combinations(sents, 2):
    fuzz.ratio(x, y)
    score.append(fuzz.ratio(x, y))
    x_list.append(x)
    y_list.append(y)
    
data_tuples = list(zip(x_list,y_list,score))

results = pd.DataFrame(data_tuples, columns=['X','Y', 'Score'])  
results = results.sort_values(by=['Score'], ascending=False)
results = results[results['Score'] > 70]
results.to_csv('SentSim.csv')

xlist = list(results['X'])
ylist = list(results['Y'])

with open("xS.txt", "wb") as x:
    x.write("\n".join(xlist).encode('utf8'))
    
with open("yS.txt", "wb") as y:
    y.write("\n".join(ylist).encode('utf8'))
